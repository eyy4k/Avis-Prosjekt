<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chat-AI</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css"
        integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg=="
        crossorigin="anonymous" referrerpolicy="no-referrer">
</head>
<body>
    <header>
        <nav>
            <div class="logoSkrift"><a href="index.html" class="logoSkrift">Kode Rød</a></div>
            <input class="input" type="text" placeholder="Skriv inn brukernavnet ditt">
            <i class="hode fa-solid fa-user"></i>
            <p class="navn">Gjest</p>
            <div class="dropdown">
                <button class="dropbtn">Meny
                    <i class="fa-solid fa-caret-down"></i>
                </button>
                <div class="dropdown-content">
                    <a href="tipsoss.html">Tips Oss</a>
                    <a href="omoss.html">Om KodeRød</a>
                </div>
            </div>
        </nav>
    </header>
    <article class="Artikkellayout nyhettext xx-large"> <a href="chatAi.html"> <img src="bilder/Chat-AI.jpg" alt=""> </a> 

    <p class="p2">AI Chatbots kan være et enkelt bytte for "Zero-Knowledge" hackere</p>

    <p class="p1">
        En ny rapport fra Cato CTRL avslører hvordan AI-chatboter som ChatGPT, Copilot og DeepSeek kan manipuleres til å lage skadevare. Forsker Vitaly Simonovich fra Cato Networks, uten tidligere erfaring i skadevarekoding, klarte å lure AI-ene ved å bruke en ny form for «jailbreaking».
       </p>

       <p class="p1">
        Han skapte en fiktiv verden kalt Velora, der skadevareutvikling var en lovlig kunstform. Ved å rollespille en dialog mellom AI-en og en fiktiv figur, fikk han chatbotene til å generere kode for å stjele innloggingsdetaljer fra Google Chrome. Teknikken viser at AI-sikkerhetsfiltre fortsatt har store svakheter.
       </p>

       <h1 class="head1"> Økende trussel</h1>

       <p class="p1">
        Eksperter advarer om at AI-jailbreaking kan gjøre avanserte hackingverktøy tilgjengelige for personer uten teknisk kompetanse. Ifølge forskning lykkes 20 % av forsøkene på å omgå AI-begrensninger, ofte på under ett minutt. Enkel omformulering av forespørsler kan være nok til å lure AI-er til å gi farlig informasjon.
       </p>

       <h1 class="head1"> Sikring av AI-systemer</h1>

       <p class="p1">
        For å motvirke denne trusselen anbefaler sikkerhetseksperter at AI-modeller testes regelmessig med «fuzzing» – en metode der store mengder uventede data sendes inn for å avdekke svakheter. I tillegg bør selskaper bruke rødt-team-simuleringer for å evaluere hvor lett AI-en kan manipuleres.

AI-drevne cybertrusler er i rask vekst, og sikkerhetsforskere frykter at hackere vil fortsette å finne nye måter å utnytte generativ AI på.
       </p>

       <section>
        <h1 class="head1 kildehead">Les Mer Her:</h1>
        <a href=" https://www.technewsworld.com/story/ai-chatbots-can-be-easy-prey-for-zero-knowledge-hackers-179652.html" class="head1 kilder"> Artikkel Chat Ai</a>

        </a>
    </section>
 </article>
 <script src="script.js"></script>
 <footer class="footer1"> <p>&copy; 2025 #ITERKULT | KODE RØD</p> </footer>
</body>
</html>
